{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Sesión 2: Transfer Learning y Fine Tuning con Keras\n",
    "\n",
    "Tutorial de Transfer Learning y Fine Tuning.  \n",
    "\n",
    "TMO: Técnicas metaheurísticas y de optimización.\n",
    "[Máster en Data Science y Big Data](http://masterds.es/) de la [Universidad de Sevilla](http://www.us.es). \n",
    "\n",
    "Profesor: [Miguel Ángel Martínez del Amor](http://www.cs.us.es/~mdelamor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de Contenidos\n",
    "\n",
    "* [1. Transfer Learning y Fine Tuning con Keras](#transfer)\n",
    "  * [1.1 Importación de librerías](#transferimp)\n",
    "  * [1.2 Descarga del Modelo en Keras](#transfermodel)\n",
    "  * [1.3 El dataset: Knifey-Spoony](#transferdata)\n",
    "  * [1.4 El canal de entrada](#transferinput)\n",
    "  * [1.5 Las clases del dataset](#transferclases)\n",
    "  * [1.6 Ejemplos de predicción](#transferej)\n",
    "  * [1.7 Transfer Learning](#transfertl)\n",
    "  * [1.8 Fine Tuning](#transferft)\n",
    "  * [1.9 Conclusiones](#transferconclu)\n",
    "* [2. Ejercicios con VGG16](#ejerciciosvgg)\n",
    "* [3. Ejercicios con Dataset Honey Bees Pollen](#ejerciciosabejas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transfer Learning y Fine Tuning con Keras <a class=\"anchor\" id=\"transfer\"></a>\n",
    "\n",
    "En este apartado del tutorial vamos a reutilizar el modelo pre-entrenado de InceptionV3 para clasificar imágenes en un conjunto de datos nuevo y distinto a ImageNet (Knifey-Spoony, ya visto en la sesión 1). Podéis ver más información sobre los modelos pre-entrenados en Keras en [este enlace](https://keras.io/applications/).\n",
    "\n",
    "Como ya hemos visto, InceptionV3 consiste en varias capas convolucionales (en realidad bloques de capas convolucionales múltiples), seguidas por algunas capas completamente conectadas (densas) y luego una capa de salida softmax para la clasificación. En la parte convolucional hay tres tipos de módulos: módulo A (con kernels de 5x5 implementados mediante kernels de 3x3, ahorrando parámetros), módulo B (con kernels de 7x7 implementados mediante kernels de 1x7), y módulo C (para alta dimensionalidad, con kernels de 3x3 implementados con kernels de 3x1).\n",
    "\n",
    "Las capas densas son responsables de combinar los rasgos de las capas convolucionales y esto ayuda en la clasificación final. Así que cuando el modelo InceptionV3 se utiliza en otro conjunto de datos, es posible que tengamos que reemplazar todas las capas densas. En este caso quitaremos la capa densa original y la reemplazaremos por otra capa densa y una capa dropout para evitar el sobreajuste.\n",
    "\n",
    "La diferencia entre Transfer Learning y Fine-Tuning es que en Transfer Learning sólo optimizamos los pesos de las nuevas capas de clasificación que hemos añadido, mientras que mantenemos los pesos del modelo original. En Fine-Tuning optimizamos tanto los pesos de las nuevas capas de clasificación que hemos añadido, como algunas o todas las capas del modelo InceptionV3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG model](images/11_inception_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importación de librerías y funciones auxiliares <a class=\"anchor\" id=\"transferimp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación la importación de la API de Keras. Comprueba como se hace la importación directamente desde TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos si estamos usando GPU desde TensorFlow. Debería aparecer `/device:GPU:X`, donde X es un número (0, 1,...) según existen más GPUs en el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Funciones auxiliares de ayuda\n",
    "\n",
    "Para unir un directorio con una lista de nombres de fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_join(dirname, filenames):\n",
    "    return [os.path.join(dirname, filename) for filename in filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dibujar imágenes, definimos esta función que usa una gráfica con hasta 9 imágenes en un grid de 3x3, y escribe la clase de verdad y las predecidas en cada imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
    "\n",
    "    assert len(images) == len(cls_true)\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "    # Adjust vertical spacing.\n",
    "    if cls_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    # Interpolation type.\n",
    "    if smooth:\n",
    "        interpolation = 'spline16'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # There may be less than 9 images, ensure it doesn't crash.\n",
    "        if i < len(images):\n",
    "            # Plot image.\n",
    "            ax.imshow(images[i],\n",
    "                      interpolation=interpolation)\n",
    "\n",
    "            # Name of the true class.\n",
    "            cls_true_name = class_names[cls_true[i]]\n",
    "\n",
    "            # Show true and predicted classes.\n",
    "            if cls_pred is None:\n",
    "                xlabel = \"True: {0}\".format(cls_true_name)\n",
    "            else:\n",
    "                # Name of the predicted class.\n",
    "                cls_pred_name = class_names[cls_pred[i]]\n",
    "\n",
    "                xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
    "\n",
    "            # Show the classes as the label on the x-axis.\n",
    "            ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para mostrar un matriz de confusión. La emplearemos para mostrar las frecuencias de las confusiones entre las clases por el predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a function from sklearn to calculate the confusion-matrix.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(cls_pred):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_test,  # True class for test-set.\n",
    "                          y_pred=cls_pred)  # Predicted class.\n",
    "\n",
    "    print(\"Matriz de confusión:\")\n",
    "    \n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "    \n",
    "    # Print the class-names for easy reference.\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(\"({0}) {1}\".format(i, class_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para mostrar ejemplos de imagenes del conjunto de test que han sido mal clasificadas (errores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_errors(cls_pred):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Boolean array whether the predicted class is incorrect.\n",
    "    incorrect = (cls_pred != cls_test)\n",
    "\n",
    "    # Get the file-paths for images that were incorrectly classified.\n",
    "    image_paths = np.array(image_paths_test)[incorrect]\n",
    "\n",
    "    # Load the first 9 images.\n",
    "    images = load_images(image_paths=image_paths[0:9])\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = cls_test[incorrect]\n",
    "    \n",
    "    # Plot the 9 images we have loaded and their corresponding classes.\n",
    "    # We have only loaded 9 images so there is no need to slice those again.\n",
    "    plot_images(images=images,\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para calcular las clases pronosticadas de todo el conjunto de test y llamar a la función anterior para dibujar algunos ejemplos de imágenes mal clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_errors(model=new_model):\n",
    "    # The Keras data-generator for the test-set must be reset\n",
    "    # before processing. This is because the generator will loop\n",
    "    # infinitely and keep an internal index into the dataset.\n",
    "    # So it might start in the middle of the test-set if we do\n",
    "    # not reset it first. This makes it impossible to match the\n",
    "    # predicted classes with the input images.\n",
    "    # If we reset the generator, then it always starts at the\n",
    "    # beginning so we know exactly which input-images were used.\n",
    "    generator_test.reset()\n",
    "    \n",
    "    # Predict the classes for all images in the test-set.\n",
    "    y_pred = model.predict_generator(generator_test,\n",
    "                                      steps=steps_test)\n",
    "\n",
    "    # Convert the predicted classes from arrays to integers.\n",
    "    cls_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "    # Plot examples of mis-classified images.\n",
    "    plot_example_errors(cls_pred)\n",
    "    \n",
    "    # Print the confusion matrix.\n",
    "    print_confusion_matrix(cls_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de ayuda para cargar imágenes. El conjunto de datos no se carga en la memoria, sino que tiene una lista de los archivos de las imágenes del conjunto de entrenamiento y otra lista de los archivos de las imágenes del conjunto de pruebas. Esta función de ayuda carga algunos archivos de imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_paths):\n",
    "    # Load the images from disk.\n",
    "    images = [plt.imread(path) for path in image_paths]\n",
    "\n",
    "    # Convert to a numpy array and return it.\n",
    "    return np.asarray(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de ayuda para trazar el historial de entrenamiento. Esto traza la precisión de clasificación y los valores de pérdida registrados durante el entrenamiento con la API de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Get the classification accuracy and loss-value\n",
    "    # for the training-set.\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    # Get it for the validation-set (we only use the test-set).\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the accuracy and loss-values for the training-set.\n",
    "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
    "    plt.plot(loss, 'o', color='b', label='Training Loss')\n",
    "    \n",
    "    # Plot it for the test-set.\n",
    "    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
    "    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
    "\n",
    "    # Plot title and legend.\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ensure the plot shows correctly.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Descarga del Modelo en Keras  <a class=\"anchor\" id=\"transfermodel\"></a>\n",
    "\n",
    "Lo siguiente crea una instancia del modelo InceptionV3 pre-entrenado usando la API de [Keras](https://keras.io/). Esto descarga automáticamente los archivos necesarios si no los tiene ya. No necestiamos el fichero `inception.py` como en la sesión 1, el cual se hizo a mano para ocultar los detalles de TensorFlow. Esta vez tenemos a Keras como capa de abstracción.\n",
    "\n",
    "El modelo InceptionV3 contiene una parte convolucional y una parte completamente conectada (o densa) que se utiliza para la clasificación. Si `include_top=True` entonces se descarga todo el modelo, que tiene unos 92 MB y unos 23 millones de parámetros. Si `include_top=False` entonces sólo se descarga la parte convolucional del modelo.\n",
    "\n",
    "Intentaremos usar el modelo pre-entrenado para predecir la clase de algunas imágenes en nuestro nuevo conjunto de datos, así que tenemos que descargar el modelo completo, pero si tienes una conexión lenta a Internet, entonces puedes modificar el código de abajo para usar el modelo pre-entrenado más pequeño sin las capas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = InceptionV3(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 El Dataset: Knifey-Spoony <a class=\"anchor\" id=\"transferdata\"></a>\n",
    "\n",
    "Volvamos a cargar el conjunto de datos Knifey-Spoony que empleamos en el apartado anterior. Recordemos que la descarga y la extracción del dataset ya viene automatizado en el fichero [`knifey.py`](./knifey.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el dataset empleando el fichero kinfey.py\n",
    "import knifey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga y extrae el dataset si aún no se ha hecho. Ocupa unos 22 MB. A continuación, mostramos el contenido de la carpeta extraida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knifey.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"El contenido de la carpeta es: \"\n",
    "!ls data/knifey-spoony\n",
    "!echo \"Por ejemplo, la carpeta forky contiene: \"\n",
    "!ls data/knifey-spoony/forky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset tiene una estructura de directoros distinta que la requerida por Keras:\n",
    "* train\n",
    " * knifey\n",
    " * forky\n",
    " * spoony\n",
    "* test\n",
    " * knifey\n",
    " * forky\n",
    " * spoony\n",
    "* val (opcional)\n",
    " * knifey\n",
    " * forky\n",
    " * spoony\n",
    "\n",
    "Por tanto copiaremos los ficheros en directorios separados para los conjuntos de pruebas (training) y pruebas (test). Esto ya está automatizado con la función copy_files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knifey.copy_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, extraemos los directorios donde están las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = knifey.train_dir\n",
    "test_dir = knifey.test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 El Canal de Entrada <a class=\"anchor\" id=\"transferinput\"></a>\n",
    "\n",
    "La API de Keras tiene su propia manera de crear el pipeline de entrada para entrenar un modelo usando ficheros.\n",
    "\n",
    "Primero necesitamos saber la forma de los tensores esperados como entrada por el modelo InceptionV3 pre-entrenado. En este caso se trata de imágenes de forma 299 x 299 x 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = model.layers[0].output_shape[1:3]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras utiliza un llamado *generador de datos* para introducir datos en la red neuronal, que iterará sobre los datos durante toda la eternidad.\n",
    "\n",
    "Tenemos un pequeño conjunto de entrenamiento, así que el generador ayudará a inflar artificialmente su tamaño haciendo varias transformaciones en las imágenes. Utilizaremos un generador de datos incorporado que puede realizar estas transformaciones aleatorias. Esto también se llama **aumentado de datos** (data augmentation).\n",
    "\n",
    "Busca en la [ayuda de Keras](https://keras.io/preprocessing/image/) qué parámetros se pueden configurar en el generador de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(\n",
    "      #rescale=1./255,\n",
    "      rotation_range=180,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "      shear_range=0.1,\n",
    "      zoom_range=[0.9, 1.5],\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      fill_mode='nearest',\n",
    "      preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También necesitamos un generador de datos para el conjunto de test, pero esto no debería hacer ninguna transformación en las imágenes porque queremos saber la precisión exacta de la clasificación en esas imágenes específicas. Así que simplemente aplicamos el preprocesamiento correspondiente al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_test = ImageDataGenerator(#rescale=1./255,\n",
    "      preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los generadores de datos devolverán batches (lotes) de imágenes. Debido a que el modelo InceptionV3 es muy grande, el tamaño del batch no puede ser demasiado grande, ya que de lo contrario nos quedaremos sin RAM en la GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar las imágenes transformadas aleatoriamente durante el entrenamiento, para comprobar si han sido demasiado distorsionadas, por lo que tendríamos que ajustar los parámetros del generador de datos anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    save_to_dir = None\n",
    "else:\n",
    "    save_to_dir='augmented_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el generador de datos real que leerá los archivos del disco, redimensionará las imágenes y devolverá un lote aleatorio.\n",
    "\n",
    "Es un poco incómodo que la construcción del generador de datos se divida en estos dos pasos, pero probablemente se deba a que hay diferentes tipos de generadores de datos disponibles para diferentes tipos de datos (imágenes, texto, etc.) y fuentes (memoria o disco)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_train = datagen_train.flow_from_directory(directory=train_dir,\n",
    "                                                    target_size=input_shape,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    save_to_dir=save_to_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El generador de datos para el conjunto de test no debería transformar y mezclar las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_test = datagen_test.flow_from_directory(directory=test_dir,\n",
    "                                                  target_size=input_shape,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que los generadores de datos iteran para siempre, necesitamos especificar el número de pasos a realizar durante la evaluación y predicción en el conjunto de pruebas. Debido a que nuestro conjunto de pruebas contiene 530 imágenes y el tamaño del batch está configurado en 20, el número de pasos es 26,5 para un procesamiento completo del conjunto de pruebas. Por eso necesitamos reiniciar el contador del generador de datos en la función `example_errors()` de arriba, para que siempre comience a procesar desde el principio del conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test = generator_test.n / batch_size\n",
    "steps_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Clases del Conjunto de Datos <a class=\"anchor\" id=\"transferclases\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos las rutas de los ficheros para todas las imágenes en los conjuntos de entrenamiento y de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train = path_join(train_dir, generator_train.filenames)\n",
    "image_paths_test = path_join(test_dir, generator_test.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos también las clasificaciones reales (el número correspondiente) de cada imagen en los conjuntos de training y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_train = generator_train.classes\n",
    "cls_test = generator_test.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos los nombres correspondientes de las clases del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(generator_train.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, obtener el número de clases en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = generator_train.num_classes\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos algunas imágenes para ver si los datos son correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carga las primeras imágenes del conjunto de training\n",
    "images = load_images(image_paths=image_paths_train[0:9])\n",
    "\n",
    "# Obtiene las clases verdades de cada una de esas imágenes.\n",
    "cls_true = cls_train[0:9]\n",
    "\n",
    "# Muestra las imágenes y las etiquetas con la función auxiliar correspondiente.\n",
    "plot_images(images=images, cls_true=cls_true, smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos Knifey-Spoony está bastante desequilibrado porque tiene pocas imágenes de tenedores, más imágenes de cuchillos y muchas más imágenes de cucharas. Esto puede causar un problema durante el entrenamiento porque a la red neural se le mostrará muchos más ejemplos de cucharas que de tenedores, por lo que puede llegar a ser mejor en el reconocimiento de las cucharas.\n",
    "\n",
    "Aquí usamos scikit-learn para calcular **pesos** que equilibrarán adecuadamente el conjunto de datos. Estos pesos se aplican al gradiente para cada imagen del lote durante el entrenamiento, de manera que se pueda escalar su influencia en el gradiente global del lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                    classes=np.unique(cls_train),\n",
    "                                    y=cls_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que el peso es de aproximadamente 1,398 para la clase tenedor y sólo 0,707 para la clase cuchara. Esto se debe a que hay menos imágenes para la clase tenedor, por lo que el gradiente debe ser amplificado para esas imágenes, mientras que el gradiente debe ser bajado para las imágenes de cuchara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Ejemplos de Predicción <a class=\"anchor\" id=\"transferej\"></a>\n",
    "\n",
    "Aquí mostraremos algunos ejemplos de uso del modelo InceptionV3 para la predicción de algunas imágenes. Necesitamos una función de ayuda para cargar y redimensionar una imagen para que pueda ser introducida en el modelo, así como para hacer la predicción real y mostrar el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path):\n",
    "    # Carga y redimensiona la imagen usando PIL.\n",
    "    img = PIL.Image.open(image_path)\n",
    "    input_shape = model.layers[0].output_shape[1:3]\n",
    "    img_resized = img.resize(input_shape, PIL.Image.LANCZOS)\n",
    "\n",
    "    # Dibuja la imagen.\n",
    "    plt.imshow(img_resized)\n",
    "    plt.show()\n",
    "\n",
    "    # Convierte la imagen PIL a un numpy-array con la forma (shape) apropiada.\n",
    "    img_array = np.expand_dims(np.array(img_resized), axis=0)\n",
    "\n",
    "    # Usa el modelo InceptionV3 para hacer la predicción.\n",
    "    # Esto devuelve un array con 1000 números, correspondientes a\n",
    "    # las clases del dataset ImageNet.\n",
    "    pred = model.predict(preprocess_input(img_array))\n",
    "    \n",
    "    # Decodifica la salida del modelo InceptionV3.\n",
    "    pred_decoded = decode_predictions(pred)[0]\n",
    "\n",
    "    # Imprime las predicciónes.\n",
    "    for code, name, score in pred_decoded:\n",
    "        print(\"{0:>6.2%} : {1}\".format(score, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usemos el modelo InceptionV3 en la foto del loro que está clasificado como guacamayo (macaw, una especie de loro) con una puntuación bastante alta de 79%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict(image_path='images/parrot_cropped1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces podemos usar el modelo InceptionV3 para predecir la clase de una de las imágenes en nuestro nuevo conjunto de entrenamiento. \n",
    "\n",
    "El modelo está muy confundido sobre esta imagen y no puede hacer una buena clasificación. Con un 13,61% de confianza cree que se trata de una red para mosquitos (mosquito_net). Quizás no le falte razón, pero está perdiendo la perspectiva del tenedor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(image_path=image_paths_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos intentarlo con otra imagen en nuestro nuevo set de entrenamiento, pero el modelo todavía está confundido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(image_path=image_paths_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos probar una imagen de nuestro nuevo conjunto de pruebas, y de nuevo el modelo InceptionV3 está muy confundido. A lo máximo que llega con un 19,11%, es una espátula (spatula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(image_path=image_paths_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Transfer Learning <a class=\"anchor\" id=\"transfertl\"></a>\n",
    "\n",
    "El modelo InceptionV3 preentrenado no pudo clasificar las imágenes del conjunto de datos Knifey-Spoony. La razón es quizás que el modelo fue entrenado en el dataset ImageNet, que puede no haber contenido muchas imágenes de cubiertos.\n",
    "\n",
    "Las capas inferiores de una Red Neural Convolucional pueden reconocer muchas formas o características diferentes en una imagen. Son las últimas capas completamente conectadas las que combinan estas características en la clasificación de una imagen completa. Así que podemos intentar redirigir la salida de la última capa convolucional del modelo InceptionV3 a una nueva red neuronal completamente conectada *(fully-connected)* que creamos para hacer la clasificación en el conjunto de datos de Knifey-Spoony.\n",
    "\n",
    "Primero imprimimos un resumen del modelo InceptionV3 para poder ver los nombres y tipos de sus capas, así como las formas de los tensores que fluyen entre las capas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = InceptionV3(include_top=False, weights='imagenet')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy sencillo crear un nuevo modelo usando la API de Keras. Primero tomamos la parte del modelo InceptionV3 desde su capa de salida (capa de transferencia). Podemos llamarlo el modelo convolucional, porque consiste en todas las capas convolucionales del modelo InceptionV3. Después aplanamos la salida convolucional con una función de pooling global, añadiremos una capa densa, después una capa de dropout para evitar sobreajuste, y finalmente la capa de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.output\n",
    "\n",
    "# Aplanamos la salida del modelo InceptionV3 dado que ésta viene\n",
    "# de una capa convolucional.\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Añade una capa densa (es decir, totalmente conectada o fully-connected).\n",
    "# Esto es para combinar las características que el modelo ha\n",
    "# reconocido en la imagen.\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Añade una capa dropout el cual prevendrá el sobreajuste y mejorará\n",
    "# la capacidad de generalización en datos desconocidos (es decir, el \n",
    "# conjunto de test)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Añade la capa final para la clasificación real, usando softmax.\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Finalmente, el modelo nuevo que vamos a entrenar\n",
    "new_model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el optimizador RMSprop con una tasa de aprendizaje bastante baja. La tasa de aprendizaje podría ser mayor, pero si se intenta entrenar más capas del modelo original InceptionV3, entonces la velocidad de aprendizaje debería ser bastante baja, de lo contrario los pesos preentrenados del modelo InceptionV3 se distorsionarán y no podrá aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 3 clases en el Knifey-Spoony dataset, por lo que Keras necesita usar esta **función de pérdida** (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La única **métrica de rendimiento** en la que estamos interesados es en la precisión de clasificación (clasiffication accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['categorical_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función auxiliar para imprimir si la capa en el modelo InceptionV3 debe ser entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_trainable():\n",
    "    for layer in model.layers:\n",
    "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, todas las capas del modelo InceptionV3 son entrenables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En transfer learning estamos inicialmente interesados tan solo en reusar el modelo InceptionV3 tal cual, como un **extractor de características**, por lo que deshabilitaremos el entrenamiento en todas sus capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayamos cambiado si las capas del modelos son entrenables, necesitamos compilarlo para que los cambios surtan efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una **época** (epoch) normalmente significa un procesamiento completo del conjunto de entrenamiento. Pero el generador de datos que creamos arriba, producirá batches de datos de entrenamiento para la eternidad. Por lo tanto, necesitamos definir el número de pasos que queremos ejecutar para cada \"época\" y este número se multiplica por el tamaño de batch definido anteriormente. En este caso tenemos 100 pasos por época y un tamaño de batch de 20, por lo que la \"época\" consiste en 2.000 imágenes aleatorias del conjunto de entrenamiento. Nosotros ejecutaremos 20 de esas \"épocas\".\n",
    "\n",
    "La razón por la que se eligieron estos números en particular, fue porque parecían ser suficientes para entrenar con este modelo y el conjunto de datos en particular, lo que no llevó mucho tiempo, y resultó en 20 puntos de datos (uno para cada \"época\") que se pueden mostrar después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps_per_epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación entrenamos el nuevo modelo, lo que se hace con tan solo una llamada a una función en la API de Keras. Esto tomará algunos minutos... (sobre unos 7 en una GTX1070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = new_model.fit_generator(generator=generator_train,\n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch=steps_per_epoch,\n",
    "                                  class_weight=class_weight,\n",
    "                                  validation_data=generator_test,\n",
    "                                  validation_steps=steps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras registra las métricas de rendimiento al final de cada \"época\" para que puedan ser mostradas en gráficos posteriormente. Esto muestra que la pérdida de valor del conjunto de entrenamiento generalmente disminuyó durante el entrenamiento, pero la pérdida de valor del conjunto de pruebas fue un poco más errática. De manera similar, la precisión de la clasificación mejoró en general en el conjunto de entrenamiento, mientras que fue un poco más errática en el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después del entrenamiento también podemos evaluar el rendimiento del nuevo modelo en el conjunto de pruebas usando una sola llamada de función en la API de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = new_model.evaluate_generator(generator_test, steps=steps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar algunos ejemplos de imágenes mal clasificadas del conjunto de pruebas. Algunas de estas imágenes también son difíciles de clasificar para un humano.\n",
    "\n",
    "La matriz de confusión muestra que el nuevo modelo está teniendo especialmente problemas para clasificar la clase *forky* (tenedor). Recordad que hemos usado sk-learn para representar la matriz, y las filas son clases verdades y las columnas clases predecidas. Esto puede ser debido a que tenemos muy pocos ejemplos de forky comparados con spoony y knifey, y aunque hayamos usado pesos de clases, no ha sido suficiente para compensar el desbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Fine-Tuning <a class=\"anchor\" id=\"transferft\"></a>\n",
    "\n",
    "En Transfer Learning, el modelo original pre-entrenado se bloquea o congela durante el entrenamiento del nuevo clasificador. Esto asegura que los pesos del modelo original InceptionV3 no cambiarán. Una ventaja de esto es que el entrenamiento del nuevo clasificador no propagará grandes gradientes a través del modelo InceptionV3, lo que puede distorsionar sus pesos o causar un sobreajuste en el nuevo conjunto de datos.\n",
    "\n",
    "Pero una vez que el nuevo clasificador ha sido entrenado, podemos tratar de afinar suavemente algunas de las capas más profundas del modelo InceptionV3 también. A esto lo llamamos \"Ajuste fino\", o **Fine Tuning**.\n",
    "\n",
    "No está claro si Keras usa el booleano `trainable` en cada capa del modelo original InceptionV3 o si es anulado por el booleano `trainable` en la'meta-capa' que llamamos `conv_layer`. Así que habilitaremos el booleano `trainable` tanto para `conv_layer` como para todas las capas relevantes en el modelo original InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos entrenar los últimos dos bloques de inception, es decir, congelamos las primeras 249 capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elegimos entrenar los dos primeros bloques de inicio, es decir, vamos a congelar\n",
    "# las primeras 249 capas y descongelar el resto:\n",
    "# en otros ejemplos se encontró que era 172 en 249. \n",
    "# Tomamos 249 de acuerdo a https://keras.io/applications/#inceptionv3\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que esto ha actualizado el booleano `trainable` para las capas relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un bajo factor de aprendizaje para el ajuste fino, por lo que los pesos del modelo InceptionV3 original solo cambiará muy lentamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_fine = RMSprop(lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que hemos definido un nuevo optimizador y hemos cambiado los booleanos `trainable` para muchas de las capas en el modelo, necesitamos recompilarlo para que los cambios hagan efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer=optimizer_fine, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos por tanto con el entrenamiento por donde lo dejamos anteriormente, ahora aplicando fine-tuning al modelo InceptionV3 y el nuevo clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = new_model.fit_generator(generator=generator_train,\n",
    "                                  epochs=30,\n",
    "                                  steps_per_epoch=steps_per_epoch,\n",
    "                                  class_weight=class_weight,\n",
    "                                  validation_data=generator_test,\n",
    "                                  validation_steps=steps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos mostrar gráficamente los valores de pérdida y precisión de la clasificación a partir del entrenamiento. Dependiendo del conjunto de datos, el modelo original, el nuevo clasificador y los hiperparámetros como la velocidad de aprendizaje, esto puede mejorar las precisiones de clasificación tanto en el conjunto de entrenamiento como en el de pruebas, o puede mejorar el conjunto de entrenamiento pero empeorarla en el conjunto de pruebas en caso de sobreajuste. Es posible que se requiera cierta experimentación con los parámetros para hacer esto bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_model.evaluate_generator(generator_test, steps=steps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos volver a mostrar algunos ejemplos de imágenes mal clasificadas, y también podemos ver en la matriz de confusión que el modelo sigue teniendo problemas para clasificar correctamente los cuchillos y las cucharas, siendo muchas veces confundidos por los tenedores.\n",
    "\n",
    "Una parte de la razón podría ser que el conjunto de entrenamiento contiene sólo 994 imágenes de tenedores, mientras que contiene 1.210 imágenes de cuchillos e imágenes de cucharas de 1.966. Aunque hemos ponderado las clases para compensar este desequilibrio, y también hemos aumentado el conjunto de entrenamiento transformando aleatoriamente las imágenes de diferentes maneras durante el entrenamiento, puede que no sea suficiente para que el modelo aprenda a reconocer correctamente los tenedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Conclusión <a class=\"anchor\" id=\"transferconclu\"></a>\n",
    "\n",
    "Este pequeño tutorial ha mostrado cómo usar la API de Keras para TensorFlow para realizar tanto transfer learning como fine tuning del modelo InceptionV3 pre-entrenado en un nuevo conjunto de datos. \n",
    "\n",
    "Que el fine tuning mejore la precisión de la clasificación en comparación con solo usar transfer learning depende del modelo pre-entrenado, de la capa de transferencia que se elija, del conjunto de datos y de la forma en que se entrena el nuevo modelo. Puede que se experimente un mejor rendimiento con el ajuste fino, o puede que experimente un peor rendimiento si el modelo ajustado está sobreajustado a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejercicios con VGG16  <a class=\"anchor\" id=\"ejerciciosvgg\"></a>\n",
    "\n",
    "Estos son algunos ejercicios que pueden ayudar a mejorar las habilidades con Keras. Para ello se propone emplear el modelo VGG16, el cual es más sencillo y que ya habéis visto en clase (o accediendo [aquí](https://github.com/fsancho/DL/blob/master/4.%20Redes%20Convolucionales/4.3.%20CNN%20Preentrenadas.ipynb)). \n",
    "\n",
    "* Utiliza el modelo VGG16 ya entrenado disponible en Keras.\n",
    "* Intenta usar otras capas en el modelo VGG16 como capa de transferencia. ¿Cómo afecta a la precisión del entrenamiento y de la clasificación?\n",
    "* Cambia las nuevas capas de clasificación que hemos añadido. ¿Puedes mejorar la precisión de la clasificación aumentando o disminuyendo el número de nodos en la capa completamente conectada / densa?\n",
    "* ¿Qué sucede si se elimina la capa de dropout en el nuevo clasificador?\n",
    "* Cambia los factores de aprendizaje tanto para el transfer learning como para el fine tuning.\n",
    "* Prueba el fine tuning en todo el modelo VGG16 en lugar de sólo las últimas capas. ¿Cómo afecta la precisión de la clasificación en los conjuntos de entrenamiento y pruebas? ¿Por qué?\n",
    "* Intenta hacer el fine tuning desde el principio para que las nuevas capas de clasificación sean entrenadas desde cero junto con todas las capas convolucionales del modelo VGGG16. Es posible que tenga que reducir la tasa de aprendizaje para el optimizador.\n",
    "* Añade algunas imágenes del conjunto de pruebas al conjunto de entrenamiento. ¿Mejora eso el rendimiento?\n",
    "* Intenta eliminar algunas de las imágenes de cuchillo y spoony del conjunto de entrenamiento para que todas las clases tengan el mismo número de imágenes. ¿Mejora eso los números en la matriz de confusión?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Transfer Learning con Keras y VGG16 <a class=\"anchor\" id=\"transfervgg\"></a>\n",
    "\n",
    "En este ejercicio a reutilizar el modelo pre-entrenado VGG16 para clasificar imágenes de un conjunto de datos nuevo. Podéis ver más información sobre los modelos pre-entrenados en Keras en [este enlace](https://keras.io/applications/).\n",
    "\n",
    "El modelo VGG16 consiste en varias capas convolucionales (en realidad bloques de capas convolucionales múltiples), seguidas por algunas capas completamente conectadas (densas) y luego una capa de salida softmax para la clasificación. En la siguiente imagen se muestra cómo construiremos un nuevo modelo con VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flowchart of Transfer Learning & Fine-Tuning](images/10_transfer_learning_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Importación de librerías y funciones auxiliares <a class=\"anchor\" id=\"transferimp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación la importación de la API de Keras. Como primer ejercicio, busca como se llama la función que carga el modelo VGG16, e importala en la celda siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Importa a continuación la función que carga el modelo VGG16\n",
    "#from keras.applications import ???\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar las mismas **funciones auxiliares** que en la sección <a href=\"#transferimp\">1.1</a>. Si no las tienes ya cargadas, por favor ve a dicha sección y carga las celdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 El Modelo Pre-Entrenado: VGG16 \n",
    "\n",
    "Lo siguiente crea una instancia del modelo VGG16 pre-entrenado usando la API de [Keras](https://keras.io/). Esto descarga automáticamente los archivos necesarios si no los tiene ya. \n",
    "\n",
    "El modelo VGG16 contiene una parte convolucional y una parte completamente conectada (o densa) que se utiliza para la clasificación. Si `include_top=True` entonces se descarga todo el modelo VGG16 que tiene unos 528 MB. Si `include_top=False` entonces sólo se descarga la parte convolucional del modelo VGG16, que es de sólo 57 MB. Descaragaremos esta última versión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG model](images/11_vgg_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vggmodel = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 El Dataset: Knifey-Spoony \n",
    "\n",
    "Carga el dataset tal y como se vió en la sección <a href=\"transferdata\">1.3</a>. Si no lo tienes ya cargado, ve dicha sección y evalúa las celdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 El Canal de Entrada\n",
    "\n",
    "Para definir el pipeline de entrada para el modelo, primero necesitamos saber la forma de los tensores esperados como entrada por el modelo VGG16 pre-entrenado. En este caso, ¿qué forma tienen las imágenes de entrada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a continuación un *generador de datos* que haga aumentado mediante transformaciones aleatorias. Para VGG16, es necesario tan solo reescalar los píxeles a 1.0/255, así que no hace falta usar la función de preprocesamiento de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ????\n",
    "datagen_test = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que el modelo VGG16 es muy grande, el tamaño del batch no puede ser demasiado grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar las imágenes transformadas aleatoriamente durante el entrenamiento, para comprobar si han sido demasiado distorsionadas, por lo que tendríamos que ajustar los parámetros del generador de datos anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    save_to_dir = None\n",
    "else:\n",
    "    save_to_dir='augmented_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el generador de datos real que leerá los archivos del disco, redimensionará las imágenes y devolverá un lote aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_train = ????\n",
    "\n",
    "generator_test = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que nuestro conjunto de pruebas contiene 530 imágenes y el tamaño del batch está configurado en 20, el número de pasos es 26,5 para un procesamiento completo del conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_test = generator_test.n / batch_size\n",
    "steps_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Clases del Conjunto de Datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos las rutas de los ficheros para todas las imágenes en los conjuntos de entrenamiento y de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train = path_join(train_dir, generator_train.filenames)\n",
    "image_paths_test = path_join(test_dir, generator_test.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos también las clasificaciones reales (el número correspondiente) de cada imagen en los conjuntos de training y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_train = generator_train.classes\n",
    "cls_test = generator_test.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos los nombres correspondientes de las clases del dataset y el número de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(generator_train.class_indices.keys())\n",
    "num_classes = generator_train.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el conjunto de datos Knifey-Spoony está bastante desequilibrado porque tiene pocas imágenes de tenedores, más imágenes de cuchillos y muchas más imágenes de cucharas. Así que vamos a calcular **pesos** que equilibrarán adecuadamente el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                    classes=np.unique(cls_train),\n",
    "                                    y=cls_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Transfer Learning\n",
    "\n",
    "Primero imprimimos un resumen del modelo VGG16 para poder ver los nombres y tipos de sus capas, así como las formas de los tensores que fluyen entre las capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggmodel.????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vamos a extraer la parte convolucional de forma personalizada, es decir, desde la entrada hasta una capa deseada (de esta forma podrás hacer transfer learning desde otras capas). Comprueba como difiere esto a como lo habéis hecho en clase anteriormente (o [aquí](https://github.com/fsancho/DL/blob/master/4.%20Redes%20Convolucionales/4.3.%20CNN%20Preentrenadas.ipynb)).\n",
    "\n",
    "Podemos ver que la última capa convolucional se llama 'block5_pool', y podemos usar Keras para obtener una referencia a dicha capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer = vggmodel.get_layer('block5_pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos referiremos a esta capa como la Capa de Transferencia (**Transfer Layer**), puesto que su salida será re-enrutada a nuestra nueva red neuronal completamente conectada que hará la clasificación final sobre el Knifey-Spoony dataset.\n",
    "\n",
    "La salida de la capa de transferencia tiene la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy sencillo crear un nuevo modelo usando la API de Keras. Primero tomamos la parte del modelo VGG16 desde su capa de entrada hasta la salida de la capa de transferencia. Podemos llamarlo el modelo convolucional, porque consiste en todas las capas convolucionales del modelo VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Model(inputs=vggmodel.input,\n",
    "                   outputs=transfer_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos entonces usar Keras para construir un modelo nuevo encima de este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos un nuevo modelo Secuencial de Keras\n",
    "nuevo_modelo = ????\n",
    "\n",
    "# Añadimos la parte convolucional del modelo VGG16 de arriba\n",
    "nuevo_modelo.add(????)\n",
    "\n",
    "# Aplanamos la salida del modelo VGG16 dado que ésta viene\n",
    "# de una capa convolucional.\n",
    "nuevo_modelo.add(????)\n",
    "\n",
    "# Añade una capa densa (es decir, totalmente conectada o fully-connected).\n",
    "# Esto es para combinar las características que el modelo VGG16 ha\n",
    "# reconocido en la imagen. Usa como función de activación ReLu.\n",
    "nuevo_modelo.add(????)\n",
    "\n",
    "# Añade una capa dropout el cual prevendrá el sobreajuste y mejorará\n",
    "# la capacidad de generalización en datos desconocidos (es decir, el \n",
    "# conjunto de test). Usa un ratio de 0.5\n",
    "nuevo_modelo.add(????)\n",
    "\n",
    "# Añade la capa final para la clasificación real, usando softmax.\n",
    "nuevo_modelo.add(????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el optimizador Adam con una tasa de aprendizaje bastante baja de 1e-5. La tasa de aprendizaje podría ser mayor, pero si se intenta entrenar más capas del modelo original VGG16, entonces la velocidad de aprendizaje debería ser bastante baja, de lo contrario los pesos preentrenados del modelo VGG16 se distorsionarán y no podrá aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 3 clases en el Knifey-Spoony dataset, por lo que Keras necesita usar una **función de pérdida** (loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La única **métrica de rendimiento** en la que estamos interesados es en la precisión de clasificación (clasiffication accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función auxiliar para imprimir si la capa en el modelo VGG16 debe ser entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_trainable():\n",
    "    for layer in conv_model.layers:\n",
    "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))\n",
    "        \n",
    "example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, todas las capas del modelo VGG16 son entrenables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En transfer learning estamos inicialmente interesados tan solo en reusar el modelo VGG16 tal cual, como un **extractor de características**, por lo que deshabilitaremos el entrenamiento en todas sus capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.???? = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in ????:\n",
    "    layer.???? = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hayamos cambiado si las capas del modelos son entrenables, necesitamos compilarlo para que los cambios surtan efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_modelo.????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación entrenamos el nuevo modelo, lo que se hace con tan solo una llamada a una función en la API de Keras. Definimos 20 épocas y 100 pasos por época (ya que tenemos batches de 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps_per_epoch = 100\n",
    "history = nuevo_modelo.????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos la gráfica de evolución de las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después del entrenamiento también podemos evaluar el rendimiento del nuevo modelo en el conjunto de pruebas usando una sola llamada de función en la API de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = nuevo_modelo.????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar algunos ejemplos de imágenes mal clasificadas del conjunto de pruebas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_errors(model=nuevo_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Fine-Tuning\n",
    "\n",
    "Podemos tratar de afinar suavemente algunas de las capas más profundas del modelo VGG16 también. A esto lo llamamos \"Ajuste fino\", o **Fine Tuning**.\n",
    "\n",
    "No está claro si Keras usa el booleano `trainable` en cada capa del modelo original VGG16 o si es anulado por el booleano `trainable` en la'meta-capa' que llamamos `conv_layer`. Así que habilitaremos el booleano `trainable` tanto para `conv_layer` como para todas las capas relevantes en el modelo original VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.???? = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos entrenar las últimas dos capas convolucionales, es decir, cuyos nombres contienen 'block5' o 'block4'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_model.????:\n",
    "    # Booleano de si la capa es entrenable\n",
    "    trainable = ('block5' in layer.name or 'block4' in layer.name)\n",
    "    \n",
    "    # Ajusta el booleano de la capa\n",
    "    layer.???? = trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que esto ha actualizado el booleano `trainable` para las capas relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el optimizador Adam con un bajo factor de aprendizaje bajo para el ajuste fino, 1e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_fine = ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que hemos definido un nuevo optimizador y hemos cambiado los booleanos `trainable` para muchas de las capas en el modelo, necesitamos recompilarlo para que los cambios hagan efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_modelo.????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos por tanto con el entrenamiento por donde lo dejamos anteriormente, ahora aplicando fine-tuning al modelo VGG16 y el nuevo clasificador. Sigamos con 20 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = nuevo_modelo.????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos mostrar gráficamente los valores de pérdida y precisión de la clasificación a partir del entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nuevo_modelo.????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos volver a mostrar algunos ejemplos de imágenes mal clasificadas, y también podemos ver en la matriz de confusión que el modelo sigue teniendo problemas para clasificar correctamente los tenedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_errors(model=nuevo_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejercicios con el Dataset Honey Bees Pollen  <a class=\"anchor\" id=\"ejerciciosabejas\"></a>\n",
    "\n",
    "Estos son algunos ejercicios sobre un nuevo conjunto de datos: [honey bees pollen](https://www.kaggle.com/ivanfel/honey-bee-pollen). En este conjunto de imágenes se grabó con cámara de vídeo a abejas transportando y sin transportar polen. Se extrajo imágenes y se recortó para que el conjunto de datos solo contenga imágenes de abejas con y sin polen. La estructura de directorios que se proporciona en el taller ya viene dispuesta dividiendo el conjunto de test y de train. Realizar los siguientes puntos:\n",
    "\n",
    "* Descarga el dataset [bees](https://www.kaggle.com/ivanfel/honey-bee-pollen). Copialo a la carpeta `data/bees`, y crea la estructura de carpetas que Keras necesita.\n",
    "* Entrena un modelo desde cero sobre este conjunto (puedes usar la red convolucional para datasets pequeños vista en clase, o [aquí](https://github.com/fsancho/DL/blob/master/4.%20Redes%20Convolucionales/4.2%20CNN%20con%20datasets%20peque%C3%B1os.ipynb). ¿Qué precisión obtienes? Es posible que tengas que adaptar algunas funciones auxiliares aquí definidas.\n",
    "* Haz transfer learning con VGG16, y después fine tuning. ¿Mejora la precisión?\n",
    "* Haz transfer Learning y fine tuning con VGG16 desde tan solo la primera capa. ¿Mejora la precisión?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carga los directorios de los datos\n",
    "train_dir = \"data/bees/train\"\n",
    "test_dir = \"data/bees/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License (MIT)\n",
    "\n",
    "Based on the TensorFlow tutorials by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\n",
    "\n",
    "Copyright (c) 2016-2017 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
