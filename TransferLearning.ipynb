{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "TransferLearning-sesion2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelamda/TL-tutorial/blob/master/TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Uk7FD2UIYdT"
      },
      "source": [
        "# Tutorial de Transfer Learning y Fine Tuning con Keras\n",
        "\n",
        "GPT2: Diseño y Gestión de Proyectos en Data Science II.\n",
        "[Máster en Data Science y Big Data](http://masterds.es/) de la [Universidad de Sevilla](http://www.us.es). \n",
        "\n",
        "25/06/2020. Profesor: [Miguel Ángel Martínez del Amor](http://www.cs.us.es/~mdelamor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jG-qgu6oIYdV"
      },
      "source": [
        "## Tabla de Contenidos\n",
        "\n",
        "* [1. Importación de librerías](#transferimp)\n",
        "* [2. El modelo InceptionV3](#inceparq)\n",
        "* [3. Ejemplos de predicción](#transferej)\n",
        "* [4. El dataset: Knifey-Spoony](#transferdata)\n",
        "* [5. El canal de entrada](#transferinput)\n",
        "* [6. Las clases del dataset](#transferclases)\n",
        "* [7. Transfer Learning](#transfertl)\n",
        "* [8. Fine Tuning](#transferft)\n",
        "* [9. Conclusiones](#transferconclu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "psTJZHZ3IYdb"
      },
      "source": [
        "\n",
        "Este tutorial muestra cómo usar la Red Neural Profunda pre-entrenada para la clasificación de imágenes. Primero, veremos una demostración detallada de cómo hacerlo, y posteriormente trabajaremos en un par de ejercicios.\n",
        "\n",
        "En concreto vamos a reutilizar el modelo pre-entrenado de InceptionV3 para un conjunto de datos nuevo y distinto a ImageNet (Knifey-Spoony). Podrás ver más información sobre los modelos pre-entrenados en Keras en [este enlace](https://keras.io/applications/).\n",
        "\n",
        "El modelo Inception v3 tardó semanas en entrenarse sobre ImageNet (formado por millones de imágenes) en un ordenador con 8 GPUs Tesla K40 que probablemente cuesta unos 30.000 euros, por lo que es casi imposible entrenarlo en un PC normal. En su lugar, descargaremos el modelo de Inception previamente entrenado y lo usaremos para clasificar las imágenes. El modelo Inception v3 tiene casi 25 millones de parámetros y utiliza 5.000 millones de operaciones de multiplicación para clasificar una sola imagen. En un PC moderno sin GPU, esto se puede hacer en una fracción de segundo por imagen.\n",
        "\n",
        "Las capas densas son responsables de combinar los rasgos de las capas convolucionales y esto ayuda en la clasificación final. Así que cuando se utiliza el modelo InceptionV3 en otro conjunto de datos, es posible que tengamos que reemplazar todas las capas densas. En este caso quitaremos la capa densa original y la reemplazaremos por otra capa densa y una capa dropout para evitar el sobreajuste.\n",
        "\n",
        "La diferencia entre Transfer Learning y Fine-Tuning es que en Transfer Learning sólo optimizamos los pesos de las nuevas capas de clasificación que hemos añadido, mientras que mantenemos los pesos del modelo original. En Fine-Tuning optimizamos tanto los pesos de las nuevas capas de clasificación que hemos añadido, como algunas o todas las capas del modelo InceptionV3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QlJOl4NsIYdd"
      },
      "source": [
        "## 1. Importación de librerías y funciones auxiliares <a class=\"anchor\" id=\"transferimp\"></a>\n",
        "\n",
        "Vamos a importar algunas librerías auxiliares y definiremos funciones que vendrán bien para trabajar con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pdd3krc8IYdW",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Antes de nada, si estás en Google Colab, evalúa esta celda:\n",
        "import os\n",
        "work_dir = \"/content/TL-tutorial/\"\n",
        "if os.getcwd() != work_dir:\n",
        "    !git clone https://github.com/miguelamda/TL-tutorial.git\n",
        "os.chdir(work_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PpxhlEd1IYdi"
      },
      "source": [
        "### 1.1 Librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PTBwMOXTIYdl",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kP-QZyJ6IYdo"
      },
      "source": [
        "A continuación la importación de la API de Keras. Comprueba como se hace la importación directamente desde TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kR5jkespIYdp",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from keras.applications import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSG6B9xCIYdt"
      },
      "source": [
        "Comprobemos si estamos usando GPU desde TensorFlow. Debería aparecer `/device:GPU:X`, donde X es un número (0, 1,...) según existen más GPUs en el sistema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OqYusRQvIYdt",
        "colab": {}
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GGmUSBHCOYVt"
      },
      "source": [
        "También podemos comprobar si tenemos una GPU de NVIDIA con el siguiente comando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKSkdvEIOXmS",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ATkLyx1CIYd5"
      },
      "source": [
        "### 1.2 Funciones auxiliares de ayuda\n",
        "\n",
        "Para unir un directorio con una lista de nombres de fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uO5HEcP5IYd6",
        "colab": {}
      },
      "source": [
        "def path_join(dirname, filenames):\n",
        "    return [os.path.join(dirname, filename) for filename in filenames]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyrJZpMrIYd9"
      },
      "source": [
        "Para dibujar imágenes, definimos esta función que usa una gráfica con hasta 9 imágenes en un grid de 3x3, y escribe la clase de verdad y las predichas en cada imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AXMRivEfIYd-",
        "colab": {}
      },
      "source": [
        "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
        "\n",
        "    assert len(images) == len(cls_true)\n",
        "\n",
        "    # Crea una figura con sub-gráficas.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "\n",
        "    # Ajusta el espacio vertical.\n",
        "    if cls_pred is None:\n",
        "        hspace = 0.3\n",
        "    else:\n",
        "        hspace = 0.6\n",
        "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
        "\n",
        "    # Tipo de interpolación.\n",
        "    if smooth:\n",
        "        interpolation = 'spline16'\n",
        "    else:\n",
        "        interpolation = 'nearest'\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Puede haber menos de 9 imágenes, nos aseguramos que no falle.\n",
        "        if i < len(images):\n",
        "            # Dibuja imagen.\n",
        "            ax.imshow(images[i],\n",
        "                      interpolation=interpolation)\n",
        "\n",
        "            # Number de la true class.\n",
        "            cls_true_name = class_names[cls_true[i]]\n",
        "\n",
        "            # Muestra clases predichas y verdaderas.\n",
        "            if cls_pred is None:\n",
        "                xlabel = \"True: {0}\".format(cls_true_name)\n",
        "            else:\n",
        "                # Nombre de la clase predicha.\n",
        "                cls_pred_name = class_names[cls_pred[i]]\n",
        "\n",
        "                xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
        "\n",
        "            # Muestra las clases con la etiqueta en el eje x.\n",
        "            ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Elimina ticks en la gráfica.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Asegurar que la gráfica se muestra correctamente con gráficos múltiples\n",
        "    # en una sola celda Notebook.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YF3Teg1fIYeG"
      },
      "source": [
        "Función para mostrar un matriz de confusión. La emplearemos para mostrar las frecuencias de las confusiones entre las clases por el predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nOupdq4YIYeH",
        "colab": {}
      },
      "source": [
        "# Importa una función de sklearn para calcular la matriz de confusión.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def print_confusion_matrix(cls_pred):\n",
        "    # cls_pred es un array del número de la clase predicha para\n",
        "    # todas las imágenes del conjunto de test.\n",
        "\n",
        "    # Obtiene la matriz de confusión usando sklearn.\n",
        "    cm = confusion_matrix(y_true=cls_test,  # True class para el conjunto de test.\n",
        "                          y_pred=cls_pred)  # Predicted class.\n",
        "\n",
        "    print(\"Matriz de confusión:\")\n",
        "    \n",
        "    # Imprime la matriz de confusión como texto.\n",
        "    print(cm)\n",
        "    \n",
        "    # Imprime los nombres de clases para facilitar la referencia.\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(\"({0}) {1}\".format(i, class_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M05WFRcHIYeM"
      },
      "source": [
        "Función para mostrar ejemplos de imagenes del conjunto de test que han sido mal clasificadas (errores)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "64K478K8IYeN",
        "colab": {}
      },
      "source": [
        "def plot_example_errors(cls_pred):\n",
        "    # cls_pred es un array del número de la clase predicha para\n",
        "    # todas las imágenes en el conjunto de test.\n",
        "\n",
        "    # Array booleano indicando si la clase predicha es incorrecta.\n",
        "    incorrect = (cls_pred != cls_test)\n",
        "\n",
        "    # Obtiene las rutas de ficheros para las imágenes que son clasificadas incorrectamente.\n",
        "    image_paths = np.array(image_paths_test)[incorrect]\n",
        "\n",
        "    # Carga las primeras 9 imágenes.\n",
        "    images = load_images(image_paths=image_paths[0:9])\n",
        "    \n",
        "    # Obtiene las clases predichas para esas imágenes.\n",
        "    cls_pred = cls_pred[incorrect]\n",
        "\n",
        "    # Obtiene las clases de verdad para esas imágenes.\n",
        "    cls_true = cls_test[incorrect]\n",
        "    \n",
        "    # Muestra las 9 imágenes que hemos cargado y sus correspondientes clases.\n",
        "    # Tenemos solo 9 imágenes, por lo que no hace falta dividirlas otra vez.\n",
        "    plot_images(images=images,\n",
        "                cls_true=cls_true[0:9],\n",
        "                cls_pred=cls_pred[0:9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ZrwxcbPIYeW"
      },
      "source": [
        "Función para calcular las clases pronosticadas de todo el conjunto de test y llamar a la función anterior para dibujar algunos ejemplos de imágenes mal clasificadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ck_zB8YzIYeY",
        "colab": {}
      },
      "source": [
        "def example_errors(model=None):\n",
        "    # El generador de datos de Keras para el conjunto de test se debe resetear\n",
        "    # antes del procesamiento. Esto es porque el generador va a iterar\n",
        "    # infintamente y mantendrá un índice interno en el dataset.\n",
        "    # Por tanto, se podrá comenzar por el medio del conjunto de test si no lo\n",
        "    # reseteamos primero. Esto imposibilita encajar las clases predichas con\n",
        "    # las imágenes de entrada. Si reseteamos el generador, entonces siempre\n",
        "    # compienza por el comienzo, así que sabemos exáctamente qué imágenes\n",
        "    # de entrada se están usando.\n",
        "    if model is None:\n",
        "        model = new_model\n",
        "        \n",
        "    generator_test.reset()\n",
        "    \n",
        "    # Predecir las clases para todas las imágenes del conjunto de test.\n",
        "    y_pred = model.predict_generator(generator_test,\n",
        "                                      steps=steps_test)\n",
        "\n",
        "    # Convertir las clases predichas de arrays a enteros.\n",
        "    cls_pred = np.argmax(y_pred,axis=1)\n",
        "\n",
        "    # Muestra los ejemplos de imágenes mal clasificados.\n",
        "    plot_example_errors(cls_pred)\n",
        "    \n",
        "    # Muestra la matriz de confusión.\n",
        "    print_confusion_matrix(cls_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i3aZqifFIYef"
      },
      "source": [
        "Función de ayuda para cargar imágenes. El conjunto de datos no se carga en la memoria, sino que tiene una lista de los archivos de las imágenes del conjunto de entrenamiento y otra lista de los archivos de las imágenes del conjunto de pruebas. Esta función de ayuda carga algunos archivos de imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3-8fjg2iIYeh",
        "colab": {}
      },
      "source": [
        "def load_images(image_paths):\n",
        "    # Carga las imágenes de disco.\n",
        "    images = [plt.imread(path) for path in image_paths]\n",
        "\n",
        "    # Convierte a un array de numpy y lo devuelve.\n",
        "    return np.asarray(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ek_MUwB8IYeq"
      },
      "source": [
        "Función de ayuda para trazar el historial de entrenamiento. Esto traza la precisión de clasificación y los valores de pérdida registrados durante el entrenamiento con la API de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "scNLgZP2IYer",
        "colab": {}
      },
      "source": [
        "def plot_training_history(history):\n",
        "    # Obtiene la precisión de clasificación y el valor de pérdida para el\n",
        "    # conjunto de entrenamiento.\n",
        "    acc = history.history['categorical_accuracy']\n",
        "    loss = history.history['loss']\n",
        "\n",
        "    # También para el conjunto de validación (solo usamos el del conjunto de test).\n",
        "    val_acc = history.history['val_categorical_accuracy']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # Muestra el valor del accuracy y pérdida para el conjunto de entrenamiento.\n",
        "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
        "    plt.plot(loss, 'o', color='b', label='Training Loss')\n",
        "    \n",
        "    # Muestra el del conjunto de test.\n",
        "    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
        "    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
        "\n",
        "    # Muestra el título y la leyenda.\n",
        "    plt.title('Training and Test Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Se asegura de mostrar la gráfica correctamente.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGzTHvuPFzH1",
        "colab_type": "text"
      },
      "source": [
        "## 2. El modelo InceptionV3 <a class=\"anchor\" id=\"inceparq\"></a>\n",
        "\n",
        "La siguiente figura muestra cómo fluyen los datos en el modelo Inception v3, que es una Red Neural Convolucional con muchas capas y una estructura complicada. El [artículo](http://arxiv.org/pdf/1512.00567v3.pdf) ofrece más detalles sobre cómo se construye el modelo Inception y por qué se diseña de esa manera. Se puede ver la arquitectura de InceptionV3, la cual consiste en varias capas convolucionales (en realidad bloques de capas convolucionales múltiples), seguidas por algunas capas completamente conectadas (densas) y luego una capa de salida softmax para la clasificación. En la parte convolucional hay tres tipos de módulos: módulo A (con kernels de 5x5 implementados mediante kernels de 3x3, ahorrando parámetros), módulo B (con kernels de 7x7 implementados mediante kernels de 1x7), y módulo C (para alta dimensionalidad, con kernels de 3x3 implementados con kernels de 3x1).\n",
        "\n",
        "Ten en cuenta que el modelo Inception tiene dos salidas softmax. Una que se usa durante el entrenamiento de la red neural como regularizador y la otra que se usa para clasificar imágenes después de que el entrenamiento haya terminado; es decir, para inferencia. Para entender mejor los detalles de la red, se puede leer este [artículo](https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYAajwLzIYdb"
      },
      "source": [
        "![11_inception_model.png](attachment:11_inception_model.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ND4P5Yc4IYeu"
      },
      "source": [
        "Lo siguiente crea una instancia del modelo InceptionV3 pre-entrenado usando la API de [Keras](https://keras.io/). Esto descarga automáticamente los archivos necesarios, si no los tiene ya en caché.\n",
        "\n",
        "Como se ha comentado antes, el modelo InceptionV3 contiene una parte convolucional y una parte completamente conectada (o densa) que se utiliza para la clasificación. Si `include_top=True` entonces se descarga todo el modelo, que tiene unos 92 MB y unos 23 millones de parámetros. Si `include_top=False` entonces sólo se descarga la parte convolucional del modelo.\n",
        "\n",
        "Intentaremos usar el modelo pre-entrenado para predecir la clase de algunas imágenes en nuestro nuevo conjunto de datos, así que tenemos que descargar el modelo completo, pero si tienes una conexión lenta a Internet, entonces puedes modificar el código de abajo para usar el modelo pre-entrenado más pequeño sin las capas de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S5uKt2SxIYev",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "model = InceptionV3(include_top=True, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Ji-ABFYIYhW"
      },
      "source": [
        "## 3. Ejemplos de predicción <a class=\"anchor\" id=\"transferej\"></a>\n",
        "\n",
        "Juguemos un poco con el modelo InceptionV3. Veamos predicciones sobre algunas imágenes de ejemplo, para entender mejor cómo funciona y cómo fue entrenado. Vamos a hacer uso de una función auxiliar que cargue y redimensione la imagen que le digamos para que pueda ser introducida en el modelo, así como para hacer la predicción real y mostrar el resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KN2kRBPzIYhX",
        "colab": {}
      },
      "source": [
        "def predict(image_path):\n",
        "    # Carga y redimensiona la imagen usando PIL.\n",
        "    img = PIL.Image.open(image_path)\n",
        "    input_shape = model.layers[0].output_shape[1:3]\n",
        "    img_resized = img.resize(input_shape, PIL.Image.LANCZOS)\n",
        "\n",
        "    # Dibuja la imagen.\n",
        "    #plt.imshow(img_resized)\n",
        "    #plt.show()\n",
        "    display(Image(image_path))\n",
        "\n",
        "    # Convierte la imagen PIL a un numpy-array con la forma (shape) apropiada.\n",
        "    img_array = np.expand_dims(np.array(img_resized), axis=0)\n",
        "\n",
        "    # Usa el modelo InceptionV3 para hacer la predicción.\n",
        "    # Esto devuelve un array con 1000 números, correspondientes a\n",
        "    # las clases del dataset ImageNet.\n",
        "    pred = model.predict(preprocess_input(img_array))\n",
        "    \n",
        "    # Decodifica la salida del modelo InceptionV3.\n",
        "    pred_decoded = decode_predictions(pred)[0]\n",
        "\n",
        "    # Imprime las predicciónes.\n",
        "    for code, name, score in pred_decoded:\n",
        "        print(\"{0:>6.2%} : {1}\".format(score, name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbsc3hmEFzH-",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Panda\n",
        "\n",
        "Esta imagen de un panda se incluye en el archivo de datos de Inception. El modelo Inception está bastante seguro de que esta imagen muestra un panda, con una puntuación de clasificación de alrededor del 89% y la siguiente puntuación más alta es de sólo alrededor del 0,8% para un indri, que es otro animal exótico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrwBfRvlFzH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path='images/cropped_panda.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO47Q51PFzIA",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Interpretación de las puntuaciones de clasificación\n",
        "\n",
        "El resultado del modelo Inception es la llamada función **Softmax**. Las salidas softmax son a veces llamadas probabilidades porque están entre cero y uno, y también suman uno - igual que las probabilidades. Pero en realidad no son probabilidades en el sentido tradicional de la palabra, porque no provienen de experimentos repetidos.\n",
        "\n",
        "Tal vez sea mejor llamar a los valores de salida de una red neuronal las *puntuaciones o rangos de clasificación*, porque indican con qué certeza cree la red que la imagen de entrada es de cada clase posible.\n",
        "\n",
        "En el ejemplo anterior con la imagen de un panda, el modelo Inception dio una puntuación muy alta de alrededor del 89% para la clase panda, y las puntuaciones para las 999 clases posibles restantes estaban todas por debajo del 1%. Esto significa que el modelo de Inception estaba bastante seguro de que la imagen mostraba un panda y que las puntuaciones restantes por debajo del 1% debían considerarse como ruido. Por ejemplo, la octava puntuación más alta fue del 0,05% para un reloj digital, pero esto se debe probablemente más a la naturaleza imprecisa de las redes neuronales que a una indicación de que la imagen se parecía ligeramente a un reloj digital.\n",
        "\n",
        "A veces el modelo Inception se confunde con la clase a la que pertenece una imagen, por lo que ninguna de las puntuaciones es realmente alta. A continuación se muestran ejemplos de ello."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3h1D5q3FzIB",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Loro (Parrot)\n",
        "#### Imagen Original\n",
        "\n",
        "El modelo Inception está muy seguro (puntuación sobre 97%) de que esta imagen muestra un tipo de loro llamado guacamayo (macaw)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivqKLvkRFzIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/parrot.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMb4EnOkFzIE",
        "colab_type": "text"
      },
      "source": [
        "#### Imagen Redimensionada\n",
        "\n",
        "El modelo Inception funciona con imágenes de entrada de 299 x 299 píxeles de tamaño. La imagen de arriba de un loro tiene en realidad 320 píxeles de ancho y 785 píxeles de alto, por lo que es redimensionada automáticamente por el modelo Inception.\n",
        "\n",
        "Veamos cómo esta imagen ha sido redimensionada por Inception antes de ser introducida al modelo, usando esta función auxiliar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyXcOogKFzIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_resized_image(image_path):\n",
        "    # Toma la imagen redimensionada del modelo Inception\n",
        "    resized_image = model.get_resized_image(image_path=image_path)\n",
        "\n",
        "    # Muestra la imagen.\n",
        "    plt.imshow(resized_image, interpolation='nearest')\n",
        "    \n",
        "    # Se asegura de que la gráfica se muestre.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glOmRgyjFzIG",
        "colab_type": "text"
      },
      "source": [
        "Esta es la imagen que se introduce en la red neuronal del modelo Inception. Podemos ver que ha sido aplastado para que sea cuadrado, y la resolución se ha reducido para que la imagen sea más pixelada y granulada.\n",
        "\n",
        "En este caso, la imagen todavía muestra claramente un loro, pero algunas imágenes pueden distorsionarse tanto a partir de este ingenuo cambio de tamaño que puede que sea mejor cambiar el tamaño de las imágenes antes de introducirlas en el modelo Inception."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eZKfD5b3FzIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_resized_image(image_path=\"images/parrot.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELWXq72TFzIK",
        "colab_type": "text"
      },
      "source": [
        "#### Imagen Recortada por arriba\n",
        "\n",
        "Esta imagen del loro se ha recortado manualmente a 299 x 299 píxeles y luego se ha introducido en el modelo Inception,\n",
        "que sigue estando muy seguro (puntuación de alrededor del 97%) de que muestra un loro (guacamayo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mGZHg1XzFzIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/parrot_cropped1.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2r_eXYOFzIN",
        "colab_type": "text"
      },
      "source": [
        "#### Imagen Recortada por el centro\n",
        "\n",
        "A continuación otro recorte de la imagen del loro, esta vez mostrando su cuerpo sin cabeza ni cola. El modelo Inception sigue teniendo mucha confianza (puntuación de alrededor del 94%) de que muestra un loro guacamayo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-J3wapK2FzIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/parrot_cropped2.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmqST7xhFzIQ",
        "colab_type": "text"
      },
      "source": [
        "#### Imagen Recortada por abajo\n",
        "\n",
        "Esta imagen ha sido recortada por lo que sólo muestra la cola del loro. Ahora el modelo Inception está bastante confundido y piensa que la imagen podría mostrar un jacamar (puntuación de alrededor del 26%) que es otro ave exótica, o quizás la imagen muestra un saltamontes (grass-hopper, puntuación de alrededor del 10%).\n",
        "\n",
        "El modelo Inception también piensa que la imagen podría mostrar una pluma estilográfica (fountain-pen, puntuación aproximada del 2%). Pero esta es una puntuación muy baja y debe interpretarse como un ruido poco fiable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad3IlAkAFzIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/parrot_cropped3.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5GJybXyFzIT",
        "colab_type": "text"
      },
      "source": [
        "#### Imagen Rellenada\n",
        "\n",
        "La mejor manera de introducir imágenes en este modelo Inception, es rellenar la imagen para que sea cuadrada (en blanco a los lados) y luego redimensionarla a 299 x 299 píxeles, como este ejemplo del loro que está clasificado correctamente con una puntuación de alrededor del 97%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2kt5S1nFzIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/parrot_padded.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDoFkDKDFzIW",
        "colab_type": "text"
      },
      "source": [
        "### 3.4 Elon Musk\n",
        "\n",
        "#### 299 x 299 píxeles\n",
        "\n",
        "Esta imagen muestra al fundador de Tesla y SpaceX, Elon Musk. Pero el modelo de Inception está muy confundido sobre lo que muestra la imagen, prediciendo que puede mostrar una sudadera (sweatshirt, puntuación alrededor del 17%) o una abaya (puntuación alrededor del 16%). También piensa que la imagen podría mostrar una pelota de ping-pong (ping-pong ball, puntuación de alrededor del 3%) o una pelota de béisbol (baseball, puntuación de alrededor del 2%). Por lo tanto, el modelo Inception está confuso y las puntuaciones de la clasificación no son fiables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuwoUdoiFzIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/elon_musk.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL5RUoO1FzIa",
        "colab_type": "text"
      },
      "source": [
        "#### 100 x 100 píxeles\n",
        "\n",
        "Si en su lugar usamos una imagen de 100 x 100 píxeles de Elon Musk, entonces el modelo de Inception piensa que podría mostrar una sudadera (sweatshirt, puntuación alrededor del 22%) o una bota de vaquero (cowboy boot, puntuación alrededor del 14%). Así que ahora el modelo Inception tiene predicciones algo diferentes, pero sigue estando muy confuso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHezFC_LFzIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/elon_musk_100x100.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBaZSjHeFzId",
        "colab_type": "text"
      },
      "source": [
        "El modelo Inception escala automáticamente el tamaño de esta imagen de 100 x 100 a 299 x 299 píxeles, que se muestra aquí. Observa lo pixelado y granuloso que realmente es, aunque un humano puede ver fácilmente que esta es la imagen de un hombre con los brazos cruzados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmPEjdGWFzId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_resized_image(image_path=\"images/elon_musk_100x100.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxhJT2VqFzIg",
        "colab_type": "text"
      },
      "source": [
        "### 3.5 Willy Wonka \n",
        "\n",
        "#### Gene Wilder\n",
        "\n",
        "Esta imagen muestra al actor Gene Wilder interpretando a Willy Wonka en la versión de la película de 1971. El modelo Inception está muy seguro de que la imagen muestra una pajarita (bow tie, puntuación de alrededor del 98%), lo que es cierto, pero un humano probablemente diría que esta imagen muestra a una persona.\n",
        "\n",
        "La razón podría ser que el modelo Inception fue entrenado en imágenes de personas con pajaritas que fueron clasificadas como pajaritas en lugar de personas. Así que tal vez el problema es que el nombre de la clase debería ser \"persona con pajarita\" en lugar de sólo \"pajarita\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImzJj_WsFzIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/willy_wonka_old.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h3vp2bCFzIi",
        "colab_type": "text"
      },
      "source": [
        "#### Johnny Depp\n",
        "\n",
        "Esta imagen muestra al actor Johnny Depp interpretando a Willy Wonka en la versión de 2005 de la película. El modelo Inception piensa que esta imagen muestra \"gafas de sol\" (sunglasses, puntuación aproximada del 34%) o \"gafa de sol\" (sunglass, puntuación aproximada del 18%). En realidad, el nombre completo de la primera clase es \"gafas de sol, gafas oscuras, sombras\". Por alguna razón, el modelo Inception ha sido entrenado para reconocer dos clases muy similares de gafas de sol. Una vez más, es cierto que la imagen muestra gafas de sol, pero un humano probablemente habría dicho que esta imagen muestra a una persona."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jvh6X0deFzIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(image_path=\"images/willy_wonka_new.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKI9fK8oFzIn",
        "colab_type": "text"
      },
      "source": [
        "### 3.6 Conclusión <a class=\"anchor\" id=\"incepconclu\"></a>\n",
        "\n",
        "En este apartado hemos demostrado cómo usar el modelo Inception v3 preentrenado. Un ordenador muy caro puede tardar varias semanas en entrenar el modelo Inception, pero podemos descargar el modelo terminado de Internet y usarlo en un PC normal para clasificar las imágenes.\n",
        "\n",
        "Desafortunadamente, el modelo Inception parece tener problemas para reconocer a las personas. Esto puede deberse al conjunto de entrenamiento que se utilizó. Ya se han publicado versiones más recientes del modelo Inception, pero es probable que también estén capacitados en el mismo conjunto de datos y, por lo tanto, también pueden tener problemas para reconocer a las personas. Se espera que los modelos futuros sean entrenados para reconocer objetos comunes como las personas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qPBQVvnFIYe1"
      },
      "source": [
        "## 4. El Dataset: Knifey-Spoony <a class=\"anchor\" id=\"transferdata\"></a>\n",
        "\n",
        "Para demostrar como usar transfer learning y fine tuning en este tutorial, usaremos un conjunto de datos llamado [Knifey-Spoony](https://github.com/Hvass-Labs/knifey-spoony) que contiene miles de imágenes de cuchillos, cucharas y tenedores sobre unos cuantos fondos diferentes. El conjunto de entrenamiento (training) contiene 4.170 imágenes y el de pruebas (test) 530 imágenes. Las clases se denominan knifey, spoony y forky (cuchillos, cucharas y tenedores) como referencia a [Los Simpsons](https://www.youtube.com/watch?v=mcE0aAhbVFc).\n",
        "\n",
        "Las imágenes en el conjunto de datos knifey-spoony fueron creadas a partir de archivos de vídeo usando un pequeño [script de Python](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/convert.py) que se ejecuta en Linux (requiere el programa `avconv` para la conversión de videos a imágenes). Esto le permite crear fácilmente conjuntos de datos muy grandes con miles de imágenes de sólo unos minutos de grabaciones de vídeo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOD2U-MsFzIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Youtube\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mcE0aAhbVFc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSc2lE1kFzIr",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Descarga del dataset\n",
        "\n",
        "La descarga y la extracción del dataset ya viene automatizado en el fichero [`knifey.py`](./knifey.py). Las dimensiones de los datos ya están definidos en el módulo `knifey`, por lo que tan solo necesitamos importarlo (asegúrate de tenerlo en la carpeta de trabajo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mMZOAB0PIYe3",
        "colab": {}
      },
      "source": [
        "# Carga el dataset empleando el fichero kinfey.py\n",
        "import knifey"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypaiEAH-IYe-"
      },
      "source": [
        "Descarga y extrae el dataset si aún no se ha hecho. Ocupa unos 22 MB. A continuación, mostramos el contenido de la carpeta extraida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XvIqE34tIYe_",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "knifey.maybe_download_and_extract()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NQhoA7Q2IYfG",
        "colab": {}
      },
      "source": [
        "!echo \"El contenido de la carpeta es: \"\n",
        "!ls data/knifey-spoony\n",
        "!echo \"Por ejemplo, la carpeta forky contiene: \"\n",
        "!ls data/knifey-spoony/forky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2JfId8b9IYfM"
      },
      "source": [
        "### 4.2 Redistribución del dataset \n",
        "\n",
        "Este dataset tiene una estructura de directoros distinta que la requerida por Keras:\n",
        "* train\n",
        " * knifey\n",
        " * forky\n",
        " * spoony\n",
        "* test\n",
        " * knifey\n",
        " * forky\n",
        " * spoony\n",
        "* val (opcional)\n",
        " * knifey\n",
        " * forky\n",
        " * spoony\n",
        "\n",
        "Por tanto copiaremos los ficheros en directorios separados para los conjuntos de pruebas (training) y pruebas (test). Esto ya está automatizado con la función copy_files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCqZpP8MIYfN",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "knifey.copy_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_BG8KFWIYfQ"
      },
      "source": [
        "Finalmente, extraemos los directorios donde están las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oF0OGOo0IYfR",
        "colab": {}
      },
      "source": [
        "train_dir = knifey.train_dir\n",
        "test_dir = knifey.test_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hBxMVp9eIYfZ"
      },
      "source": [
        "## 5. El Canal de Entrada <a class=\"anchor\" id=\"transferinput\"></a>\n",
        "\n",
        "La API de Keras tiene su propia manera de crear el pipeline de entrada para entrenar un modelo usando ficheros.\n",
        "\n",
        "Primero necesitamos saber la forma de los tensores esperados como entrada por el modelo InceptionV3 pre-entrenado. En este caso se trata de imágenes de forma 299 x 299 x 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HV5Z9s_KIYfa",
        "colab": {}
      },
      "source": [
        "input_shape = model.layers[0].output_shape[1:3]\n",
        "input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7V-2aKkaIYfc"
      },
      "source": [
        "Keras utiliza un llamado *generador de datos* para introducir datos en la red neuronal, que iterará sobre los datos durante toda la eternidad.\n",
        "\n",
        "Tenemos un pequeño conjunto de entrenamiento, así que el generador ayudará a inflar artificialmente su tamaño haciendo varias transformaciones en las imágenes. Utilizaremos un generador de datos incorporado que puede realizar estas transformaciones aleatorias. Esto también se llama **aumentado de datos** (data augmentation).\n",
        "\n",
        "Busca en la [ayuda de Keras](https://keras.io/preprocessing/image/) qué parámetros se pueden configurar en el generador de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "23uer5SNIYfd",
        "colab": {}
      },
      "source": [
        "datagen_train = ImageDataGenerator(\n",
        "      #rescale=1./255, # No hace falta normalizar, esto ya lo hace preprocess_input de inception_v3\n",
        "      rotation_range=180,\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.1,\n",
        "      zoom_range=[0.9, 1.5],\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True,\n",
        "      fill_mode='nearest',\n",
        "      preprocessing_function=preprocess_input)  # Esta función es específica para inception_v3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0LG80rlsIYff"
      },
      "source": [
        "También necesitamos un generador de datos para el conjunto de test, pero esto no debería hacer ninguna transformación en las imágenes porque queremos saber la precisión exacta de la clasificación en esas imágenes específicas. Así que simplemente aplicamos el preprocesamiento correspondiente al modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rRwxlQZzIYfg",
        "colab": {}
      },
      "source": [
        "datagen_test = ImageDataGenerator(#rescale=1./255,\n",
        "      preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a5PlOUvuIYfn"
      },
      "source": [
        "Los generadores de datos devolverán batches (lotes) de imágenes. Debido a que el modelo InceptionV3 es muy grande, el tamaño del batch no puede ser demasiado grande, ya que de lo contrario nos quedaremos sin RAM en la GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "juAcZOIFIYfp",
        "colab": {}
      },
      "source": [
        "batch_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fmbz-TTiIYft"
      },
      "source": [
        "Podemos guardar las imágenes transformadas aleatoriamente durante el entrenamiento, para comprobar si han sido demasiado distorsionadas, por lo que tendríamos que ajustar los parámetros del generador de datos anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC89Hvq4IYfu",
        "colab": {}
      },
      "source": [
        "if True:\n",
        "    save_to_dir = None\n",
        "else:\n",
        "    save_to_dir='augmented_images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lS7010sNIYfy"
      },
      "source": [
        "Ahora creamos el generador de datos real que leerá los archivos del disco, redimensionará las imágenes y devolverá un lote aleatorio.\n",
        "\n",
        "Es un poco incómodo que la construcción del generador de datos se divida en estos dos pasos, pero probablemente se deba a que hay diferentes tipos de generadores de datos disponibles para diferentes tipos de datos (imágenes, texto, etc.) y fuentes (memoria o disco)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H470b6WiIYfz",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "generator_train = datagen_train.flow_from_directory(directory=train_dir,\n",
        "                                                    target_size=input_shape,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                    save_to_dir=save_to_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PFSllacCIYf7"
      },
      "source": [
        "El generador de datos para el conjunto de test no debería transformar y mezclar las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EiM3T2oNIYf7",
        "colab": {}
      },
      "source": [
        "generator_test = datagen_test.flow_from_directory(directory=test_dir,\n",
        "                                                  target_size=input_shape,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_xsD6PuuIYf9"
      },
      "source": [
        "Debido a que los generadores de datos iteran para siempre, necesitamos especificar el número de pasos a realizar durante la evaluación y predicción en el conjunto de pruebas. Debido a que nuestro conjunto de pruebas contiene 530 imágenes y el tamaño del batch está configurado en 20, el número de pasos es 26,5 para un procesamiento completo del conjunto de pruebas. Por eso necesitamos reiniciar el contador del generador de datos en la función `example_errors()` de arriba, para que siempre comience a procesar desde el principio del conjunto de pruebas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7r67G16IYf-",
        "colab": {}
      },
      "source": [
        "steps_test = generator_test.n / batch_size\n",
        "steps_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lkjRmGazIYgA"
      },
      "source": [
        "## 6. Clases del Conjunto de Datos <a class=\"anchor\" id=\"transferclases\"></a>\n",
        "\n",
        "A continuación vamos a obtener las clases del dataset, y vamos a visualizar qué hace Inception cuando intenta predecir su clasificación. ¿Será capaz de detectar una cuchara?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IVjFid5aIYgB"
      },
      "source": [
        "Obtengamos las rutas de los ficheros para todas las imágenes en los conjuntos de entrenamiento y de pruebas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WK3nIgn-IYgB",
        "colab": {}
      },
      "source": [
        "image_paths_train = path_join(train_dir, generator_train.filenames)\n",
        "image_paths_test = path_join(test_dir, generator_test.filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OxqwOnP5IYgE"
      },
      "source": [
        "Obtengamos también las clasificaciones reales (el número correspondiente) de cada imagen en los conjuntos de training y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "af-KDyF0IYgE",
        "colab": {}
      },
      "source": [
        "cls_train = generator_train.classes\n",
        "cls_test = generator_test.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yea4Bb-_IYgP"
      },
      "source": [
        "Obtengamos los nombres correspondientes de las clases del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F8_Dna-lIYgQ",
        "colab": {}
      },
      "source": [
        "class_names = list(generator_train.class_indices.keys())\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Al7AVF-zIYgU"
      },
      "source": [
        "Por último, obtener el número de clases en el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cq3N6owMIYgV",
        "colab": {}
      },
      "source": [
        "num_classes = generator_train.num_classes\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J5zUVBzxIYge"
      },
      "source": [
        "Mostremos algunas imágenes para ver si los datos son correctos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLAZYOw5IYgh",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Carga las primeras imágenes del conjunto de training\n",
        "images = load_images(image_paths=image_paths_train[0:9])\n",
        "\n",
        "# Obtiene las clases verdaderas de cada una de esas imágenes.\n",
        "cls_true = cls_train[0:9]\n",
        "\n",
        "# Muestra las imágenes y las etiquetas con la función auxiliar correspondiente.\n",
        "plot_images(images=images, cls_true=cls_true, smooth=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FmSIs1y-IYgz"
      },
      "source": [
        "El conjunto de datos Knifey-Spoony está bastante desequilibrado porque tiene pocas imágenes de tenedores, más imágenes de cuchillos y muchas más imágenes de cucharas. Esto puede causar un problema durante el entrenamiento porque a la red neural se le mostrará muchos más ejemplos de cucharas que de tenedores, por lo que puede llegar a ser mejor en el reconocimiento de las cucharas.\n",
        "\n",
        "Aquí usamos scikit-learn para calcular **pesos** que equilibrarán adecuadamente el conjunto de datos. Estos pesos se aplican al gradiente para cada imagen del lote durante el entrenamiento, de manera que se pueda escalar su influencia en el gradiente global del lote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Wn13mJ-IYg1",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nfLbKC54IYhB",
        "colab": {}
      },
      "source": [
        "class_weight = compute_class_weight(class_weight='balanced',\n",
        "                                    classes=np.unique(cls_train),\n",
        "                                    y=cls_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GT4eiFMTIYhG"
      },
      "source": [
        "Observa que el peso es de aproximadamente 1,398 para la clase tenedor y sólo 0,707 para la clase cuchara. Esto se debe a que hay menos imágenes para la clase tenedor, por lo que el gradiente debe ser amplificado para esas imágenes, mientras que el gradiente debe ser bajado para las imágenes de cuchara."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "818YHGlaIYhG",
        "colab": {}
      },
      "source": [
        "class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tw7hDae1IYhN",
        "colab": {}
      },
      "source": [
        "class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PdZvxDwDIYhz"
      },
      "source": [
        "Ahora sí, podemos usar el modelo InceptionV3 para predecir la clase de una de las imágenes en nuestro nuevo conjunto de entrenamiento. \n",
        "\n",
        "El modelo está muy confundido sobre esta imagen y no puede hacer una buena clasificación. Con un 11,79% de confianza cree que se trata de una red para mosquitos (mosquito_net). Quizás no le falte razón, pero está perdiendo la perspectiva del tenedor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aryCpHtNIYh3",
        "colab": {}
      },
      "source": [
        "predict(image_path=image_paths_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "276iJQ73IYiB"
      },
      "source": [
        "Podemos intentarlo con otra imagen en nuestro nuevo set de entrenamiento, pero el modelo todavía está confundido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IiBruwgXIYiC",
        "colab": {}
      },
      "source": [
        "predict(image_path=image_paths_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YdLRj1f0IYiF"
      },
      "source": [
        "También podemos probar una imagen de nuestro nuevo conjunto de pruebas, y de nuevo el modelo InceptionV3 está muy confundido. A lo máximo que llega con un 12,23%, es una espátula (spatula)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jgRslJ-6IYiG",
        "colab": {}
      },
      "source": [
        "predict(image_path=image_paths_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WtleL4w4T_xB"
      },
      "source": [
        "Podemos concluir que el modelo InceptionV3 pre-entrenado no nos vale para trabajar con este nuevo dataset. Pero quizás podamos reutilizar parte del aprendizaje para esta nueva tarea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bBN31p23IYiK"
      },
      "source": [
        "## 7. Transfer Learning <a class=\"anchor\" id=\"transfertl\"></a>\n",
        "\n",
        "El modelo InceptionV3 preentrenado no pudo clasificar las imágenes del conjunto de datos Knifey-Spoony. La razón es quizás que el modelo fue entrenado en el dataset ImageNet, que puede no haber contenido muchas imágenes de cubiertos.\n",
        "\n",
        "Las capas inferiores de una Red Neural Convolucional pueden reconocer muchas formas o características diferentes en una imagen. Son las últimas capas completamente conectadas las que combinan estas características en la clasificación de una imagen completa. Así que podemos intentar redirigir la salida de la última capa convolucional del modelo InceptionV3 a una nueva red neuronal completamente conectada *(fully-connected)* que creamos para hacer la clasificación en el conjunto de datos de Knifey-Spoony.\n",
        "\n",
        "Primero imprimimos un resumen del modelo InceptionV3 para poder ver los nombres y tipos de sus capas, así como las formas de los tensores que fluyen entre las capas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VYSdGr4EIYiK",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "model = InceptionV3(include_top=False, weights='imagenet')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hu0mKcSzIYiU"
      },
      "source": [
        "Es muy sencillo crear un nuevo modelo usando la API de Keras. Primero tomamos la parte del modelo InceptionV3 desde su capa de salida (capa de transferencia). Podemos llamarlo el modelo convolucional, porque consiste en todas las capas convolucionales del modelo InceptionV3. Después aplanamos la salida convolucional con una función de pooling global, añadiremos una capa densa, después una capa de dropout para evitar sobreajuste, y finalmente la capa de clasificación. Puedes ver en Keras qué funciones hay que utilizar para hacer transfer learning: https://keras.io/api/applications/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VTkTD_84IYiV",
        "colab": {}
      },
      "source": [
        "x = model.output\n",
        "\n",
        "# Aplanamos la salida del modelo InceptionV3 dado que ésta viene\n",
        "# de una capa convolucional.\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Añade una capa densa (es decir, totalmente conectada o fully-connected).\n",
        "# Esto es para combinar las características que el modelo ha\n",
        "# reconocido en la imagen.\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# Añade una capa dropout el cual prevendrá el sobreajuste y mejorará\n",
        "# la capacidad de generalización en datos desconocidos (es decir, el \n",
        "# conjunto de test)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Añade la capa final para la clasificación real, usando softmax.\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Finalmente, el modelo nuevo que vamos a entrenar\n",
        "new_model = Model(inputs=model.input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "evC9-fzLIYiZ"
      },
      "source": [
        "Por ejemplo, podemos utilizar el optimizador RMSprop con una tasa de aprendizaje bastante baja. La tasa de aprendizaje podría ser mayor, pero si se intenta entrenar más capas del modelo original InceptionV3, entonces la velocidad de aprendizaje debería ser bastante baja, de lo contrario los pesos preentrenados del modelo InceptionV3 se distorsionarán y no podrá aprender."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CLZ8os_YIYia",
        "colab": {}
      },
      "source": [
        "optimizer = RMSprop(lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8D79XN9IYij"
      },
      "source": [
        "Tenemos 3 clases disjuntas en el Knifey-Spoony dataset. Es decir, estamos en un problema de clasificación multiclase, así que necesitamos usar la siguiente **función de pérdida** (loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A6pZTaDlIYin",
        "colab": {}
      },
      "source": [
        "loss = 'categorical_crossentropy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uyEp1hrnIYiy"
      },
      "source": [
        "La única **métrica de rendimiento** en la que estamos interesados es en la precisión de clasificación (clasiffication accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IsdGmvbnIYiy",
        "colab": {}
      },
      "source": [
        "metrics = ['categorical_accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vT7CpvaQIYi2"
      },
      "source": [
        "Función auxiliar para imprimir si la capa en el modelo InceptionV3 debe ser entrenada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_P91GaKGIYi3",
        "colab": {}
      },
      "source": [
        "def print_layer_trainable():\n",
        "    for layer in model.layers:\n",
        "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-BdQlPFIYi6"
      },
      "source": [
        "Por defecto, todas las capas del modelo InceptionV3 son entrenables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1r1jKyCgIYi7",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print_layer_trainable()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1PNcl7TsIYjE"
      },
      "source": [
        "En transfer learning estamos inicialmente interesados tan solo en reusar el modelo InceptionV3 tal cual, como un **extractor de características**, por lo que deshabilitaremos el entrenamiento en todas sus capas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MHxsUgJ6IYjF",
        "colab": {}
      },
      "source": [
        "model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5OB6geTRIYjK",
        "colab": {}
      },
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ls8-5Xm2IYjP",
        "colab": {}
      },
      "source": [
        "print_layer_trainable()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4HRhYVZxIYjb"
      },
      "source": [
        "Una vez que hayamos cambiado si las capas del modelos son entrenables, necesitamos compilarlo para que los cambios surtan efecto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cvXiuZcWIYjc",
        "colab": {}
      },
      "source": [
        "new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yjwwAWX-IYjj"
      },
      "source": [
        "Una **época** (epoch) normalmente significa un procesamiento completo del conjunto de entrenamiento. Pero el generador de datos que creamos arriba, producirá batches de datos de entrenamiento para la eternidad. Por lo tanto, necesitamos definir el número de pasos que queremos ejecutar para cada \"época\" y este número se multiplica por el tamaño de batch definido anteriormente. En este caso tenemos 100 pasos por época y un tamaño de batch de 20, por lo que la \"época\" consiste en 2.000 imágenes aleatorias del conjunto de entrenamiento. Nosotros ejecutaremos 20 de esas \"épocas\".\n",
        "\n",
        "La razón por la que se eligieron estos números en particular, fue porque parecían ser suficientes para entrenar con este modelo y el conjunto de datos en particular, lo que no llevó mucho tiempo, y resultó en 20 puntos de datos (uno para cada \"época\") que se pueden mostrar después."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QglEg8XLIYjk",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "steps_per_epoch = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NYm7mhLKIYjo"
      },
      "source": [
        "A continuación entrenamos el nuevo modelo, lo que se hace con tan solo una llamada a una función en la API de Keras. Esto tomará algunos minutos... (sobre unos 15 en una T4 en Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7BT9T3QGIYjp",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "history = new_model.fit_generator(generator=generator_train,\n",
        "                                  epochs=epochs,\n",
        "                                  steps_per_epoch=steps_per_epoch,\n",
        "                                  class_weight=class_weight,\n",
        "                                  validation_data=generator_test,\n",
        "                                  validation_steps=steps_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O85tvB2yIYjr"
      },
      "source": [
        "Keras registra las métricas de rendimiento al final de cada \"época\" para que puedan ser mostradas en gráficos posteriormente. Esto muestra que la pérdida de valor del conjunto de entrenamiento generalmente disminuyó durante el entrenamiento, pero la pérdida de valor del conjunto de pruebas fue un poco más errática. De manera similar, la precisión de la clasificación mejoró en general en el conjunto de entrenamiento, mientras que fue un poco más errática en el conjunto de pruebas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cuRAtL1QIYjr",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "plot_training_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBjxVVYgIYju"
      },
      "source": [
        "Después del entrenamiento también podemos evaluar el rendimiento del nuevo modelo en el conjunto de pruebas usando una sola llamada de función en la API de Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9eHM1TAyIYju",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "result = new_model.evaluate_generator(generator_test, steps=steps_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F6EIkz5wIYj1",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6HPUkZtIYj7"
      },
      "source": [
        "Podemos representar algunos ejemplos de imágenes mal clasificadas del conjunto de pruebas. Algunas de estas imágenes también son difíciles de clasificar para un humano.\n",
        "\n",
        "La matriz de confusión muestra que el nuevo modelo está teniendo especialmente problemas para clasificar la clase *forky* (tenedor). Recordad que hemos usado sk-learn para representar la matriz, y las filas son clases verdades y las columnas clases predecidas. Esto puede ser debido a que tenemos muy pocos ejemplos de forky comparados con spoony y knifey, y aunque hayamos usado pesos de clases, no ha sido suficiente para compensar el desbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nBW4hQdhIYj8",
        "colab": {}
      },
      "source": [
        "example_errors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ELuCFr6nIYj_"
      },
      "source": [
        "## 8. Fine-Tuning <a class=\"anchor\" id=\"transferft\"></a>\n",
        "\n",
        "En Transfer Learning, el modelo original pre-entrenado se bloquea o congela durante el entrenamiento del nuevo clasificador. Esto asegura que los pesos del modelo original InceptionV3 no cambiarán. Una ventaja de esto es que el entrenamiento del nuevo clasificador no propagará grandes gradientes a través del modelo InceptionV3, lo que puede distorsionar sus pesos o causar un sobreajuste en el nuevo conjunto de datos.\n",
        "\n",
        "Pero una vez que el nuevo clasificador ha sido entrenado, podemos tratar de afinar suavemente algunas de las capas más profundas del modelo InceptionV3 también. A esto lo llamamos \"Ajuste fino\", o **Fine Tuning**.\n",
        "\n",
        "No está claro si Keras usa el booleano `trainable` en cada capa del modelo original InceptionV3 o si es anulado por el booleano `trainable` en la'meta-capa' que llamamos `conv_layer`. Así que habilitaremos el booleano `trainable` tanto para `conv_layer` como para todas las capas relevantes en el modelo original InceptionV3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rfJX2hJIYkA",
        "colab": {}
      },
      "source": [
        "model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TT3nZFzMIYkD"
      },
      "source": [
        "Queremos entrenar los últimos dos bloques de inception, es decir, congelamos las primeras 249 capas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bbr7phOIYkD",
        "colab": {}
      },
      "source": [
        "# elegimos entrenar los dos primeros bloques de inicio, es decir, vamos a congelar\n",
        "# las primeras 249 capas y descongelar el resto:\n",
        "# en otros ejemplos se encontró que era 172 en 249. \n",
        "# Tomamos 249 de acuerdo con https://keras.io/applications/\n",
        "for layer in model.layers[249:]:\n",
        "    layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0W333kzbIYkI"
      },
      "source": [
        "Podemos comprobar que esto ha actualizado el booleano `trainable` para las capas relevantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oMsXaGTVIYkI",
        "colab": {}
      },
      "source": [
        "print_layer_trainable()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vf_MFUiYIYkN"
      },
      "source": [
        "Usaremos un factor de aprendizaje bajo para el ajuste fino, para que los pesos del modelo InceptionV3 original solo cambien muy lentamente. No conviene permitir que estos pesos cambien demasiado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGE1WqCBIYkN",
        "colab": {}
      },
      "source": [
        "optimizer_fine = RMSprop(lr=1e-6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O6BgFzwxIYkW"
      },
      "source": [
        "Dado que hemos definido un nuevo optimizador y hemos cambiado los booleanos `trainable` para muchas de las capas en el modelo, necesitamos recompilarlo para que los cambios hagan efecto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "77_BZbBfIYkX",
        "colab": {}
      },
      "source": [
        "new_model.compile(optimizer=optimizer_fine, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Utg6WNANIYkb"
      },
      "source": [
        "Continuamos por tanto con el entrenamiento por donde lo dejamos anteriormente, ahora aplicando fine-tuning al modelo InceptionV3 y el nuevo clasificador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXOk8xpmIYkb",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "history = new_model.fit_generator(generator=generator_train,\n",
        "                                  epochs=30,\n",
        "                                  steps_per_epoch=steps_per_epoch,\n",
        "                                  class_weight=class_weight,\n",
        "                                  validation_data=generator_test,\n",
        "                                  validation_steps=steps_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "52re8tYyIYkf"
      },
      "source": [
        "Luego podemos mostrar gráficamente los valores de pérdida y precisión de la clasificación a partir del entrenamiento. Dependiendo del conjunto de datos, el modelo original, el nuevo clasificador y los hiperparámetros como la velocidad de aprendizaje, esto puede mejorar las precisiones de clasificación tanto en el conjunto de entrenamiento como en el de pruebas, o puede mejorar el conjunto de entrenamiento pero empeorarla en el conjunto de pruebas en caso de sobreajuste. Es posible que se requiera cierta experimentación con los parámetros para hacer esto bien."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zNQFgsYbIYkg",
        "colab": {}
      },
      "source": [
        "plot_training_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wg0C4pN3IYko",
        "colab": {}
      },
      "source": [
        "result = new_model.evaluate_generator(generator_test, steps=steps_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RAFKcMx2IYku",
        "colab": {}
      },
      "source": [
        "print(\"Test-set classification accuracy: {0:.2%}\".format(result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bs5BVTYCIYk4"
      },
      "source": [
        "Podemos volver a mostrar algunos ejemplos de imágenes mal clasificadas, y también podemos ver en la matriz de confusión que el modelo sigue teniendo problemas para clasificar correctamente los cuchillos y las cucharas, siendo muchas veces confundidos por los tenedores.\n",
        "\n",
        "Una parte de la razón podría ser que el conjunto de entrenamiento contiene sólo 994 imágenes de tenedores, mientras que contiene 1.210 imágenes de cuchillos e imágenes de cucharas de 1.966. Aunque hemos ponderado las clases para compensar este desequilibrio, y también hemos aumentado el conjunto de entrenamiento transformando aleatoriamente las imágenes de diferentes maneras durante el entrenamiento, puede que no sea suficiente para que el modelo aprenda a reconocer correctamente los tenedores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWqJsChkIYk7",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "example_errors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MaR7rHtEIYlI"
      },
      "source": [
        "## 9. Conclusión <a class=\"anchor\" id=\"transferconclu\"></a>\n",
        "\n",
        "Este pequeño tutorial ha mostrado cómo usar la API de Keras para TensorFlow para realizar tanto transfer learning como fine tuning del modelo InceptionV3 pre-entrenado en un nuevo conjunto de datos. \n",
        "\n",
        "Que el fine tuning mejore la precisión de la clasificación en comparación con solo usar transfer learning depende del modelo pre-entrenado, de la capa de transferencia que se elija, del conjunto de datos y de la forma en que se entrena el nuevo modelo. Puede que se experimente un mejor rendimiento con el ajuste fino, o puede que experimente un peor rendimiento si el modelo ajustado está sobreajustado a los datos de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "myuiuKyUIYoT"
      },
      "source": [
        "## License (MIT)\n",
        "\n",
        "Based on the TensorFlow tutorials by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\n",
        "\n",
        "Copyright (c) 2016-2017 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}